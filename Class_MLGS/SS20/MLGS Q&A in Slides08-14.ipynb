{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A in MLGS slides - Part 2\n",
    "*Also some summations to the questions in the slides.*\n",
    "\n",
    "*Feel free to revise it if there is something wrong.*\n",
    "\n",
    "*@author Riemann Lee*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. Sequential - Neural Nets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In an RNN, the hidden state at a given time influences all hidden states into the future. However, an RNN cannot model long-term dependencies. Why?\n",
    "\n",
    "2. What is the rceptive field of a causal convolution and dilate convolution with $n$ layers?\n",
    "\n",
    "**Answer-NN**\n",
    "1. The impact of future times might vanish or explode (e.g. 1-D example: $W>1$ or $W<1$), thus RNN cannot retain information for many steps.\n",
    "\n",
    "2. With causal convolution we use $(n+1)$ inputs in the first layer while in dilate convolution we use $2^n$ inputs in the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Sequential - Temporal Point Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is it possible to obtain the conditional intensity $\\lambda^*(t)$ if you know only the survival function $S^*(t)$ and don't know the conditional PDF $p^*(t)$?\n",
    "2. Which process would you use to model the following event data? (a) Hawkes process, (b) inhomogeneous Poisson process\n",
    "    * Customers visiting a supermarkt(event = customer enters the supermarkt)\n",
    "    * Messages sent by a single user on WhatsApp(event = message sent) \n",
    "    * Taxi rides in a city(event = a trip starts)\n",
    "3. What can you say about a TPP with the following conditional intensity function? What kind of behaviour does it model?\n",
    "$$\\lambda^*(t) = exp(t-\\sum \\limits_{t_i \\in \\mathcal{H}(t)} 1)$$\n",
    "\n",
    "**Answer-TPP**\n",
    "1. Yes, see Slides09-9\n",
    "2. \n",
    "    * Inhomogeneous Poisson process: one customer entering the supermarket does not make it likelier that someone follows.\n",
    "    * Hawkes process, since it is benificial for clustered/bursty event occurences.\n",
    "    * Inhomogeneous Poisson process: If someone starts a trip has no influence on someone else starting a trip.\n",
    "3. See two versions of explainations:  \n",
    "Abstract from **Piazza**: As the the time to the last event observed increases, so does the probability of seeing the next event.  \n",
    "Summation from **live Q&A-09**:\n",
    "    - if there is no events happening, then the intensity just keep going up, which means grows exponentially\n",
    "    - if there is some events, then the intensity drops down:\n",
    "     * it it's very unlikely that you have 2 events one after another. As soon as one event happens, then we have to wait sometime until the intensity accumulate again\n",
    "     * just think of it as the opposite of Hawkes process, namely, self-correcting process, we can use it to explain the situation of earthquake. Let's say stress accumulates in the plate, then it release and leads to a earthquake, then stress goes down, wait for a second time for another release. Globally, it's just like the behaviour of earthquakes.\n",
    "     * In practice, we use NN to model earthquakes because you can model everything together. Here just simple models with simple scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Graphs - Graphs Networks & Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Graph & Networks\n",
    "1. How much memory do you need to store the edges of a graph with 1000 nodes and 10,000 edges in a dense adjacency matrix? How much for a sparse matrix?\n",
    "2. What is the average degree in an Erdös-Renyi graph with edge probability $p$? And in a real world sparse graph with $O(E) = O(N^\\alpha)$\n",
    "\n",
    "**Answer-Graphs & Neworks**\n",
    "C.f. [*Piazza-384*](https://piazza.com/class/k8a8g6pwk2i4xe?cid=384)\n",
    "1. For a dense adjacency matrix, we need $1000 * 1000$ memory to store all the $0$ or $1$ data, and this has nothing to do with the number of actual edges, so it's $O(1,000,000)$. But for a sparse matrix, we only need to consider the number of edges since here we only need to consider the edges and its corresponding positions regardless of other zero elements, so it's $O(10,000)$.\n",
    "\n",
    "2. For Edrös-Renyi graph, there are total $C_N^2$ possible edges in a graph with $N$ nodes, each node enjoy at most $N-1$ degree, so the average degree is $(N-1)p$. In a real world graph the number of edges scale slower than quadratically as $E=c \\cdot N^\\alpha$ for some $\\alpha<2$ and a positive constant $c$. Each edge involves two nodes and on average we get \n",
    "$\\frac{2c \\cdot N^\\alpha}{N} = 2c\\cdot N^{\\alpha−1}$. Because $ 1≤\\alpha \\leq2$,  $\\alpha−1 \\leq 1$ and the average degree in a real world graph grows as $O(\\sqrt[\\alpha]{N})$ for some a that depends on α."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Generative Models\n",
    "1. Can the Erdös-Renyi model generate all graphs that the Stochastic Block Model can generate?\n",
    "2. Is the initial Attractiveness model equal to the Erdös-Renyi model as $A \\rightarrow \\infty$?\n",
    "3. Why does $\\pi$ not need to be normalized for the Gumebl trick to work?\n",
    "\n",
    "**Answer-Generative Models**\n",
    "C.f. [*Piazza-384*](https://piazza.com/class/k8a8g6pwk2i4xe?cid=384)\n",
    "1. Yes, because each edge has positive probablity so an ER model can generate any possible graph. It is just very unlikely to generate graphs with community structures etc.\n",
    "2. No. As we approach the limit of infinite initial attractiveness the probability of generating an edge $p(a,b)$ approaches the uniform distribution for some nodes $a$ and $b$ that we assume were inserted in iterations $a$ and $b$ ($a$ and $b$ are natural numbers and node identifiers). In particular $p(a,b)=\\frac{1}{b−1}$ if $a<b$ and assuming $m=1$. If we now insert node $b+1$, the probability of creating an edge with a is $p(a,b+1)=\\frac{1}{b}$. But these two probabilities are different, therefore the model is not an ER model.\n",
    "3. Assume that $\\pi = c\\cdot \\hat{\\pi}$ is proportional to the normalized parameters $\\hat{\\pi}$. Then $log \\pi_i= log \\hat{\\pi_i} + log c$. Obviously, the argmax is insensitive with respect to constant additions but the same is true of the softmax operation. To see that write out the definition of softmax for a vector $a_i+c$ for some constant $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Graphs - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. How can you find the connected components of a graph from its Laplacian?\n",
    "2. Consider a graph with $n$ arbitary connected nodes and $k$ disconnected nodes. What are the first $k+1$ clusters that spectral clustering finds? Why?\n",
    "\n",
    "**Answer-Clustering**\n",
    "1. Abstract from *Piazza*: We know we can assign each connected component with an indicator vector such that it's the eigenvector of the zero eigenvalue. In order to find the connected components, we find all eigenvectors which correspond to zero eigenvalue.\n",
    "\n",
    "2. The important point of the second question is that no matter how big the largest connected component of your graph is, if you don't filter out singleton nodes first, your spectral clustering/embedding will be useless. [scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/fd237278e895b42abe8d8d09105cbb82dc2cbba7/sklearn/manifold/_spectral_embedding.py#L235) even checks for this and warns you if you have multiple connected components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Graphs - Node Embeddings & Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Node Embeddings\n",
    "1. How can you use node embeddings to visualize the structure of a graph?\n",
    "2. Consider two nodes $u$ and $v$ in a graph that share the same nodes attributes $x$ but are far apart in the graph. What can you say about the embeddings that Graph2Gauss would find for these nodes? Why do other methods work better?\n",
    "\n",
    "**Answer-Node Embeddings**\n",
    "1. C.f. [Piazza-431](https://piazza.com/class/k8a8g6pwk2i4xe?cid=431): We explicitly use node information when using Graph2Gauss method. At the same time, the edge information is implicitly used since we are looking at triplets where $v$ is closer to $u$ w.r.t $w$, which shows closer distance in terms of the graph, i.e. edges/hops etc.\n",
    "2. C.f. [Piazza-455](https://piazza.com/class/k8a8g6pwk2i4xe?cid=455): Graph2Gauss incorperate both nodes attribute and distance information, thus:\n",
    "    - if you have nodes sharing the same node attributes, the embeddings will also be the same\n",
    "    - these will be similar embeddings (in terms of KL Divergence) for nodes which are closer in the original graph and very different embeddings for nodes which are far apart.  \n",
    "    - So when it is more important to incorporate distance information, Spectral Embeddings and DeepWalk which rely only the graph structure would be better; if more importance is attached to the node attributes, Graph2Gauss from above would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Ranking\n",
    "Consider a directed cycly of length 3 as a Markov chain disregarding edge weights:\n",
    "![Directed cycle of length 3](note_img/Pic_Q_Rank.png)\n",
    "* Is it reducible? Is it aperiodic?\n",
    "* How does the introduction of random teleports change the above 3-cycle?\n",
    "* How can you make it aperiodic by inserting just a single edge?\n",
    "\n",
    "**Answer-Ranking**\n",
    "- reducible but periodic\n",
    "- In addition to the original transition probabilities, Random Teleports introduce a jump to random state, thus all the states are now aperiodic.\n",
    "- We can achieve this by adding a self-point edge at any of these 3 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the graph below: ![Example](note_img/Pic_Q_Semi.png)What's the influence of the green node on the unlabeled nodes in Label Propagation? Why? How about GNNs with $K=2$?\n",
    "2. Does semi-supervised learning exist outside of learning on graphs?\n",
    "3. What could be alternative aggregation functions beside summation in the Gather step of GNNs?\n",
    "4. Can you apply GNNs to vector data without a specified graph structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Limitations of GNNs\n",
    "### Question:Overview\n",
    "1. The Weisfeiler-Lehman test works similar to message passing with hashing as an aggregation function. Why don't we just use hash aggregation in GNNs to lift them to the same level of expressiveness?\n",
    "![non-isomorphic examples](note_img/Pic_Q_Limit_1.png)\n",
    "2. Above you see two non-isomotphic graphs that the WL test cannot distinguish, can you make this counter example smaller?\n",
    "3. Why is oversmoothing a problem for node-level prediction models?\n",
    "\n",
    "**Answer-Overview** C.f. [Piazza-461](https://piazza.com/class/k8a8g6pwk2i4xe?cid=461)\n",
    "1. Depending on the hash function, the output can be quite arbitrary so it does not really preserve the features of the individual nodes; \n",
    "2. See sketch in Slides.\n",
    "3. By learning a global representation, GCN loses the ability to represent local neighbourhoods. Therefore, in a node-level prediction, where we are mainly interested in individual nodes, this would be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Robustness\n",
    "1. Is a Projected Gradient Descent attack on a GNN via the graph structure a good idea? Why or why not?\n",
    "2. Suppose you want to determine the worse-case structure perturbation $\\Delta$, which is limited to (i)insert or (ii)remove at most k edges. How many possible perturbations are there(in big-O notation w.r.t. the number of nodes $N$ and the number of edges $E$)?\n",
    "3. Given a graph with 2810 nodes and 7336 edges. What value of $p_{\\alpha}$ do we need to choose if in expectation we want to sample 7336 further edges?\n",
    "\n",
    "**Answer-Robustness**\n",
    "1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
