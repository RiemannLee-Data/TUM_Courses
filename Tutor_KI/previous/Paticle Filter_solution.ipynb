{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4, Part 2: Robot localization using particle filtering\n",
    "## Introduction\n",
    "In the lecture we have learned particle filtering. which can be interpreted as Monte Carlo method for Hidden Markov Models. In this exercise, we learn how to use this method to track a moving robot's position overtime. Basically, we have access to the steering and velocity control inputs. We also have sensors that measures distance to visible landmarks. Then the basic idea is that the population of particles tracks the high-likelihood regions of the robot's position.\n",
    "\n",
    "Your tasks is to complete the missing code. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring.\n",
    "Do not add or modify any code outside of the following comment blocks:\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook. You are **NOT** allowed to using additional 'import'  statements. If you plagiarise even for a single project task, you won't be eligible for the bonus this semester.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interactive\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# comment these two lines if you don't want multiple output in a cell\n",
    "# just for the convenience of debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Constuct particles\n",
    "Particles can be constructed by uniformly sampling in the 2D space, or by Gaussian sampling in robot's start point. The latter one is more helpful if you know where the robot's start point is. In this notebook, we have no information about robot's start point, so we just randomly sample particles over the 2D space. In this task, you need to sample particles in the given 2D space with no landmarks, namely, all randomly sampled particles is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_particles_construction(width, height, N):\n",
    "    \"\"\"Sample particles in 2D space with no landmarks randomly.\n",
    "\n",
    "    Args:\n",
    "        width, height: int, width and height of the area in which the robot moves\n",
    "        N: int, number of particles\n",
    "    \n",
    "    Return:\n",
    "        particles: np.ndarray, the coordinate of particles in 2D space\n",
    "    \"\"\"\n",
    "    np.random.seed(500)\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show a example map in which the robot moves:\n",
    "\n",
    "<img src=\"./img/Map_Rejection_Sampling.png\" alt=\"Encoder\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Find valid particles\n",
    "The map above contains many irregulately distributed landmarks shown as blue circles. In this case, not all previous sampled particles are valid since these particles can not located inside the landmarks. In this task you need to find the valid particles, namely, only those particles whose distance to each landmarks' center is larger or equal to the landmarks' corresponding radius is considered as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_particles(particles, centers, radius):\n",
    "    \"\"\"Given randomly sampled particles, as well as centers and radii of landmarks, decide which particles are valid.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: centers of landmarks\n",
    "        radius: the radius of cicular landmarks \n",
    "    Return:\n",
    "        valid_particles: the particles that locate in the valid region\n",
    "    \"\"\"\n",
    "    valid_particles = []\n",
    "\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    for count, particle_coord in enumerate(particles):\n",
    "        dis = np.linalg.norm(particle_coord-centers, axis=1, keepdims=True)\n",
    "        if np.all(dis >= radius):\n",
    "            valid_particles.append(particles[count])    \n",
    "    ###################################################################\n",
    "    return np.asarray(valid_particles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Now we have got the uniformly sampled particles in the 2D space and also select the valid particles. Since we have access to the velocity and angle of the robot's motion, now we can move the remaining valid particles based on how you predict the real system is behaving with different noise level in the motion model. Then we can have a look at the influence of different noise level. Assuming that our sensors returns the data in a time interval of 0.5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_particle_pos(particles, v, std, dt=0.4):\n",
    "    \"\"\"Predict the motion of next state for each particles given current angles and velocities. Note that the preiction here is noised.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejecting the ones that are not valid\n",
    "        vï¼š 2d array. Each sample with feature [angle, velocity]\n",
    "        std: control the noise of the distance between time steps\n",
    "        dt: time interval, according to sensor model, it should be 0.4 second here\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    \n",
    "    # std can be set as a hyperparameter to decide how noisy is the data, thus can change difficulty\n",
    "    # add some noise to the distance, the level is control in the parameter dis_noise_args\n",
    "    delta_dist = (v[1] * dt) + (np.random.randn(N) * std) \n",
    "    particles[:, 0] += np.cos(v[0]) * delta_dist\n",
    "    particles[:, 1] += np.sin(v[0]) * delta_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights of each particle\n",
    "Update the weighting of the particles based on the measurement. Each particle has a position and a weight which estimates how well it matches the measurement. Normalizing the weights so they sum to one. This normalization step turns them into a probability distribution. Those particles that are closer to the robot will generally have a higher weight than ones far from the robot. Particles that closely match the measurements are weighted higher than particles which don't match the measurements very well. So in this case, we can measure the probability using the distance to landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_update(particles, weights, dist_r_l, centers, radii, scale_fac, std):\n",
    "    \"\"\"Given the noised distances from robot to the landmarks, update the weights of particles\n",
    "    \n",
    "    Args:\n",
    "        particles: coordinate of particles\n",
    "        weights: weight of particles\n",
    "        dist_r_l: the current distance between robot and landmarks\n",
    "        scale_fac, std: hyperparameters to avoid the underflow of possibilities\n",
    "    \"\"\"\n",
    "    \n",
    "    weights.fill(1.)\n",
    "    \n",
    "    for count, center in enumerate(centers):\n",
    "        # distance between the particles and each landmark\n",
    "        dist_p_l = np.linalg.norm(particles-center, axis=1, keepdims=True) - radii[count]\n",
    "        \n",
    "        weights *= stats.norm.pdf(dist_p_l/scale_fac, dist_r_l[count]/scale_fac, std)\n",
    "\n",
    "    weights += 1.e-100   # avoid round-off to zero\n",
    "    weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Resample Procedures\n",
    "Discard highly impossible particles and replace them with copies of the more possible particles. Accordingly, particles with greater weights survive with higher likelihood than particles with valus of small importance. In principle, there are many ways to achieve this, here you can refer to the procedure given in the paper [Resampling in Particle Filtering - Comparison](http://sait.cie.put.poznan.pl/38/SAIT_38_02.pdf) to complete the systematic resampling method, in which the algorithm is described as below:\n",
    "\n",
    "<img src=\"img/systematic_resample.png\" alt=\"Encoder\" style=\"width: 400px;\"/>\n",
    "\n",
    "This resampling has a complexity of *O(N)*, and is one of the more readily recommended, because of its simplicity and operation speed. This approach assumes that the range $[0, 1)$ is subdivided in to N equal parts, and the draw occurs in each stratum\n",
    "$u^i \\sim [\\frac{i-1}{N}, \\frac{i}{N})$, particles are selected for replication, such that $u^i \\in [\\sum_p^{j-1}q_p, \\sum_p^{j}q_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_procedure(weights):\n",
    "    \"\"\"Perform resampling procedure described above\n",
    "    \n",
    "    Args:\n",
    "        weights: the weights to be updated\n",
    "    \n",
    "    ReturnL:\n",
    "        idx: the indices of those remained particled\n",
    "    \"\"\"\n",
    "    \n",
    "    num_weights = len(weights)\n",
    "    \n",
    "    idx = np.zeros(num_weights, 'i') # set the data type as int\n",
    "    sum_Q = np.cumsum(weights)\n",
    "\n",
    "    # make N subdivisions, choose positions with a consistent random offset\n",
    "    U = (np.arange(num_weights) + np.random.uniform()) / num_weights\n",
    " \n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    i, j = 0, 0\n",
    "    while i<num_weights and j<num_weights:\n",
    "        if U[i] < sum_Q[j]:\n",
    "            idx[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    ###################################################################\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above takes an array of weights and returns indexes of particles that have been chosen. We just need to write a function that performs the resampling from these indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, idx):\n",
    "    particles[:] = particles[idx]\n",
    "    weights[:] = weights[idx]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information about the maps\n",
    "landmark_centers = np.load('./data/centers/example_centers.npy')\n",
    "landmark_radii = np.load('./data/radii/example_radii.npy')\n",
    "# input the origin data as reference, you should not touch this data in you implementation\n",
    "input_sequence = np.load('./data/trajectory/example_trajectory.npy')\n",
    "# Now let's input the velocity and distance data, which should be intepreted\n",
    "# as corresponding transition and observability matrix in HMM type\n",
    "angle_velocity = np.load('./data/velocity/example_velocity.npy')\n",
    "dist_r_l = np.load('./data/distance/example_distance.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparametes\n",
    "width_max = 800\n",
    "height_max = 600 \n",
    "particle_number = 500\n",
    "\n",
    "# add noise to the distance prediction\n",
    "dis_noise_args = {'no noise': 0, 'low noise': 1, 'high noise': 5}\n",
    "\n",
    "# decide the convergence of particles\n",
    "weight_conv_args = {'scale': 50, 'std':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_trajectory_record(width, height, N, centers, radii, velocity, distance, noise, weighting):\n",
    "    # First we initialize the particles and the weights\n",
    "    random_particles = uniform_particles_construction(width, height, N)\n",
    "    remaining_particles = find_valid_particles(random_particles, centers, radii)\n",
    "    origin_random_weights = np.ones((len(remaining_particles),1))\n",
    "\n",
    "    # Now we need to record the coordinates of moving particles\n",
    "    prediction_particles = [remaining_particles]\n",
    "\n",
    "    random_pos = copy.copy(remaining_particles)\n",
    "    random_weights = copy.copy(origin_random_weights)\n",
    "\n",
    "    for i in range(len(distance)):\n",
    "\n",
    "        # The predicted position of particles\n",
    "        predict_particle_pos(random_pos, velocity[i], noise, 0.4)\n",
    "        weights_update(random_pos, random_weights, distance[i], centers, radii, weighting['scale'], weighting['std'])\n",
    "        random_index = resample_procedure(random_weights)\n",
    "        resample_from_index(random_pos, random_weights, random_index)\n",
    "        prediction_particles.append(copy.copy(random_pos))\n",
    "    \n",
    "    particle_trajectory = np.asarray(prediction_particles)\n",
    "    return particle_trajectory, random_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_no_noise, weight_no_noise = particle_trajectory_record(width_max, height_max, particle_number, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args['no noise'], weight_conv_args)\n",
    "\n",
    "trajectory_low_noise, weight_low_noise = particle_trajectory_record(width_max, height_max, particle_number, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args['low noise'], weight_conv_args)\n",
    "\n",
    "trajectory_high_noise, weight_high_noise = particle_trajectory_record(width_max, height_max, particle_number, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args['high noise'], weight_conv_args)"
   ]
  },
  {
   "source": [
    "## Visulization\n",
    "Now we visualize the moving of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(step):\n",
    "    img = plt.imread('img/Canvas.png')\n",
    "    fig,ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax_unpack = ax.ravel()\n",
    "    \n",
    "    ax_unpack[0].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax_unpack[0].scatter(trajectory_no_noise[step,:,0], trajectory_no_noise[step,:,1],s=1, c='g')\n",
    "\n",
    "    ax_unpack[1].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax_unpack[1].scatter(trajectory_low_noise[step,:,0], trajectory_low_noise[step,:,1],s=1, c='g')\n",
    "\n",
    "    ax_unpack[2].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax_unpack[2].scatter(trajectory_high_noise[step,:,0], trajectory_high_noise[step,:,1],s=1, c='g')\n",
    "    # Now, loop through coord arrays, and create a circle at center\n",
    "    for i in range(0, 3):\n",
    "        ax_unpack[i].set_aspect('equal')\n",
    "        for count, value in enumerate(landmark_centers):\n",
    "            circ = Circle(value,landmark_radii[count], alpha=0.5)\n",
    "            ax_unpack[i].add_patch(circ)\n",
    "        ax_unpack[i].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=25, description='step', max=50), Output()), _dom_classes=('widget-interaâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9913f429040c491a8957c37d0f759e20"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "iplot = interactive(location, step=(0, len(trajectory_no_noise)-1))\n",
    "iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's predict the position of robot's final position and make comparison with the true result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The estimate position with no noise is [618.85110432 480.22242078]\nThe estimate position with low lowise is [611.03029821 484.30986893]\nThe estimate position with high noise is [607.02302265 493.04249201]\nThe true final position of robot is [615.12953368 500.55248619]\n"
     ]
    }
   ],
   "source": [
    "position_no_noise = np.average(trajectory_no_noise[-1], weights=weight_no_noise.flatten(), axis=0)\n",
    "position_low_noise = np.average(trajectory_low_noise[-1], weights=weight_low_noise.flatten(), axis=0)\n",
    "position_high_noise = np.average(trajectory_high_noise[-1], weights=weight_high_noise.flatten(), axis=0)\n",
    "print(\"The estimate position with no noise is {}\".format(position_no_noise))\n",
    "print(\"The estimate position with low lowise is {}\".format(position_low_noise))\n",
    "print(\"The estimate position with high noise is {}\".format(position_high_noise))\n",
    "print(\"The true final position of robot is {}\".format(input_sequence[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the resule\n",
    "We can see that the prediction of final position with no noise is not always the best result. Sometimes the prediction with some noise works better. You can relate it to the the gradient descent method when we want to find the minumum value of a non-convex. For example if we get stuck in some local minimum, performing random walks helps to eacape from this local minima. Same is hold for particle filters with different noise levels. Once the minimum offset for particles with no noise is decided, it will always be there and can never be eliminated. But with different noise level, particles may come to some smaller offset "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}