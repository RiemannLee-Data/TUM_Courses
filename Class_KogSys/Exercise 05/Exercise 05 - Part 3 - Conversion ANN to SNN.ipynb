{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 05 - Part 3 - Conversion ANN to SNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GM6VSs5kT_fy",
        "oBPIQK3mVPbH",
        "8bC0-TCqVYaH",
        "Zql-qjTzWEIo",
        "t4qLVXcvbK9F",
        "P3HnSH3CbeSd"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6VSs5kT_fy",
        "colab_type": "text"
      },
      "source": [
        "# End-to-end example for SNN Toolbox.\n",
        "\n",
        "This script sets up a small CNN using Keras and tensorflow, trains it for one\n",
        "epoch on MNIST, stores model and dataset in a temporary folder on disk, creates\n",
        "a configuration file for SNN toolbox, and finally calls the main function of\n",
        "SNN toolbox to convert the trained ANN to an SNN and run it using Brian2\n",
        "simulator.\n",
        "\n",
        "Taken from: https://github.com/NeuromorphicProcessorProject/snn_toolbox/blob/master/examples/mnist_keras_brian2.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdZw4zUtT_Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare colab by installing neccessary modules:\n",
        "!pip install snntoolbox\n",
        "!pip install brian2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mYF0K7T-wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all other imports\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, \\\n",
        "    Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from snntoolbox.bin.run import main\n",
        "from snntoolbox.utils.utils import import_configparser"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBPIQK3mVPbH",
        "colab_type": "text"
      },
      "source": [
        "# 1. Define a working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwtryoCwUy4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define path where model and output files will be stored.\n",
        "# The user is responsible for cleaning up this temporary directory.\n",
        "path_wd = 'snn_toolbox_example_{}'.format(str(time.time()))\n",
        "os.makedirs(path_wd)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bC0-TCqVYaH",
        "colab_type": "text"
      },
      "source": [
        "# 2. Get the MNIST data set\n",
        "\n",
        "![MNIST](https://miro.medium.com/max/1060/1*VAjYygFUinnygIx9eVCrQQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pWt7pqOWC2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize input so we can train ANN with it.\n",
        "# Will be converted back to integers for SNN layer.\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Add a channel dimension.\n",
        "axis = 1 if keras.backend.image_data_format() == 'channels_first' else -1\n",
        "x_train = np.expand_dims(x_train, axis)\n",
        "x_test = np.expand_dims(x_test, axis)\n",
        "\n",
        "# One-hot encode target vectors.\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Save dataset so SNN toolbox can find it.\n",
        "np.savez_compressed(os.path.join(path_wd, 'x_test'), x_test)\n",
        "np.savez_compressed(os.path.join(path_wd, 'y_test'), y_test)\n",
        "# SNN toolbox will not do any training, but we save a subset of the training\n",
        "# set so the toolbox can use it when normalizing the network parameters.\n",
        "np.savez_compressed(os.path.join(path_wd, 'x_norm'), x_train[::10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zql-qjTzWEIo",
        "colab_type": "text"
      },
      "source": [
        "# 3. Define ANN with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTdVLnR4WZVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This section creates a simple CNN using Keras, and trains it\n",
        "# with backpropagation. There are no spikes involved at this point.\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "input_layer = Input(input_shape)\n",
        "\n",
        "layer = Conv2D(filters=16,\n",
        "               kernel_size=(5, 5),\n",
        "               strides=(2, 2))(input_layer)\n",
        "layer = BatchNormalization(axis=axis)(layer)\n",
        "layer = Activation('relu')(layer)\n",
        "layer = Conv2D(filters=32,\n",
        "               kernel_size=(3, 3),\n",
        "               activation='relu')(layer)\n",
        "layer = AveragePooling2D()(layer)\n",
        "layer = Conv2D(filters=8,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same',\n",
        "               activation='relu')(layer)\n",
        "layer = Flatten()(layer)\n",
        "layer = Dropout(0.01)(layer)\n",
        "layer = Dense(units=10,\n",
        "              activation='softmax')(layer)\n",
        "\n",
        "model = Model(input_layer, layer)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile('adam', 'categorical_crossentropy', ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4qLVXcvbK9F",
        "colab_type": "text"
      },
      "source": [
        "# 4. Train and save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ns2JiJbKUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model with backprop.\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Store model so SNN Toolbox can find it.\n",
        "model_name = 'mnist_cnn'\n",
        "keras.models.save_model(model, os.path.join(path_wd, model_name + '.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3HnSH3CbeSd",
        "colab_type": "text"
      },
      "source": [
        "# 5. Configure SNN toolbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTgWutGBbew9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a config file with experimental setup for SNN Toolbox.\n",
        "configparser = import_configparser()\n",
        "config = configparser.ConfigParser()\n",
        "\n",
        "config['paths'] = {\n",
        "    'path_wd': path_wd,             # Path to model.\n",
        "    'dataset_path': path_wd,        # Path to dataset.\n",
        "    'filename_ann': model_name      # Name of input model.\n",
        "}\n",
        "\n",
        "config['tools'] = {\n",
        "    'evaluate_ann': True,           # Test ANN on dataset before conversion.\n",
        "    'normalize': True,              # Normalize weights for full dynamic range.\n",
        "}\n",
        "\n",
        "config['simulation'] = {\n",
        "    'simulator': 'brian2',          # Chooses execution backend of SNN toolbox.\n",
        "    'duration': 50,                 # Number of time steps to run each sample.\n",
        "    'num_to_test': 5,               # How many test samples to run.\n",
        "    'batch_size': 1,                # Batch size for simulation.\n",
        "    'dt': 0.1                       # Time resolution for ODE solving.\n",
        "}\n",
        "\n",
        "config['input'] = {\n",
        "    'poisson_input': False          # Images are encodes as spike trains.\n",
        "}\n",
        "\n",
        "config['output'] = {\n",
        "    'plot_vars': {                  # Various plots (slows down simulation).\n",
        "        'spiketrains',              # Leave section empty to turn off plots.\n",
        "        'spikerates',\n",
        "        'activations',\n",
        "        'correlation',\n",
        "        'v_mem',\n",
        "        'error_t'}\n",
        "}\n",
        "\n",
        "# Store config file.\n",
        "config_filepath = os.path.join(path_wd, 'config')\n",
        "with open(config_filepath, 'w') as configfile:\n",
        "    config.write(configfile)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeXoqP1Fb2dc",
        "colab_type": "text"
      },
      "source": [
        "# 6. Run the SNN toolbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rneb_SpOOLfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main(config_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}