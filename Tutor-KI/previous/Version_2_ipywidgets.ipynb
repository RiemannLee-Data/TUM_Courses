{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot localization using particle filter\n",
    "The robot has steering and velocity control inputs. It has sensors that measures distance to visible landmarks. Both the sensors and control mechanism have noise in them, and we need to track the robot's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interactive\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# comment these two lines if you don't want multiple output in a cell\n",
    "# just for the convenience of debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constuct particles\n",
    "Particles can be constructed by randomly sampling in the 2D space, or by Gaussian sampling in robot's start point. The latter one is more helpful if you know where the robot's start point is. But in this scene, we have no information about robot's start point, so we just randomly sample particles over the 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_particles_construction(width, height, N):\n",
    "\n",
    "    # set the random seed so that we have reproducible experiments\n",
    "    np.random.seed(500)\n",
    "\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_particles_construction(width, height, N, sigma):\n",
    "    particles = sigma * np.random.randn(N, 2) + (width/2, height/2)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reject sampled particles\n",
    "Here we use the random sampling method as illustration. Since previously we get particles randomly among the total 2D spaces, it is possible that there are some particles located inside landmarks, thus we neet to delete these invalid particles whose distance to the landmarks is less than the corresponding radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_particles(particles, centers, radius):\n",
    "    \"\"\"Given randomly sampled particles and centers of landmarks, perform rejection here\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: centers of landmarks\n",
    "        radius: the radius of cicular landmarks \n",
    "    \"\"\"\n",
    "    particles_after_rejection = []\n",
    "    for count_p, coord_p in enumerate(particles):\n",
    "        dis = np.linalg.norm(coord_p-centers, axis=1, keepdims=True)\n",
    "        if np.all(dis >= radius):\n",
    "            particles_after_rejection.append(particles[count_p])\n",
    "    return np.asarray(particles_after_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predefine some parameters and then run two examples to see the influence of number of evidence. What do you see in these two examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated acceptability for 1000 particles is 0.99700\n",
      "Estimated acceptability for 2000 particles is 0.99750\n",
      "Estimated acceptability for 3000 particles is 0.99767\n",
      "Estimated acceptability for 4000 particles is 0.99750\n",
      "Estimated acceptability for 5000 particles is 0.99700\n",
      "Estimated acceptability for 6000 particles is 0.99717\n",
      "Estimated acceptability for 7000 particles is 0.99657\n",
      "Estimated acceptability for 8000 particles is 0.99625\n",
      "Estimated acceptability for 9000 particles is 0.99644\n",
      "The true acceptability is 0.99585\n"
     ]
    }
   ],
   "source": [
    "[width_max, height_max]= [800, 600]\n",
    "\n",
    "# Landmark center coordinates\n",
    "\n",
    "# Version 1: 5 landmarks\n",
    "landmark_centers = np.array([ [336,175], [718,159], [510,43], [167, 333], [472, 437] ])\n",
    "landmark_radii = np.array([[12],[6],[7],[18],[9]])\n",
    "\n",
    "# Version 2: ten landmarks\n",
    "# centers = np.array([ [144,73], [510,43], [336,175], [718,159], [178,484], [665,464], [267, 333], [541, 300], [472, 437], [100, 533] ])\n",
    "# radii = np.array([[12],[32],[7],[8],[13],[6],[7],[8],[9],[10]])\n",
    "\n",
    "if len(landmark_centers) != len(landmark_radii):\n",
    "    raise ValueError(\"centers and radii must have the same size!\")\n",
    "\n",
    "num_particles = np.arange(1000, 10000, 1000)\n",
    "\n",
    "for i in range(len(num_particles)):\n",
    "    particles = uniform_particles_construction(width_max, height_max, num_particles[i])\n",
    "    rejection =  rejection_particles(particles, landmark_centers, landmark_radii)\n",
    "    print('Estimated acceptability for {} particles is {:.5f}'.format(num_particles[i], len(rejection[:,0])/len(particles[:,0])))\n",
    "\n",
    "print('The true acceptability is {:.5f}'.format(1 - (np.pi * landmark_radii * landmark_radii).sum() / (width_max * height_max) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Now we can move the remaining particles based on how you predict the real system is behaving with some noise in the motion model. Assuming that out sensors returns the data in a time interval of 0.5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_particle_pos(particles, v, std, dt=0.5):\n",
    "    \"\"\"Predict the motion of next state for each particles given current angles and velocities.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejecting the ones that are not available\n",
    "        vï¼š 2d array. Each sample with feature [angle, velocity]\n",
    "        std: standard deviation of velociy\n",
    "        dt: time interval, assume it to be 0.5 second here\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    \n",
    "    # std can be set as a hyperparameter to decide how noisy is the data, thus can change difficulty\n",
    "    delta_dist = (v[1] * dt) + (np.random.randn(N) * std) # add some noise to the distance\n",
    "    # delta_dist = (v[1] * dt)\n",
    "    particles[:, 0] += np.cos(v[0]) * delta_dist\n",
    "    particles[:, 1] += np.sin(v[0]) * delta_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights of each particle\n",
    "Update the weighting of the particles based on the measurement. Each particle has a position and a weight which estimates how well it matches the measurement. Normalizing the weights so they sum to one. This normalization step turns them into a probability distribution. Those particles that are closer to the robot will generally have a higher weight than ones far from the robot. Particles that closely match the measurements are weighted higher than particles which don't match the measurements very well. So in this case, we can measure the probability using the distance to landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_update(particles, weights, dist_r_l, centers, radii, scale_fac, std):\n",
    "    \"\"\"Given the noised distances from robot to the landmarks, update the weights of particles\n",
    "    \n",
    "    Args:\n",
    "        particles: coordinate of particles\n",
    "        weights: weight of particles\n",
    "        dist_r_l: the current distance between robot and landmarks\n",
    "        scale_fac, std: hyperparameters to avoid the underflow of possibilities\n",
    "    \"\"\"\n",
    "    \n",
    "    weights.fill(1.)\n",
    "    \n",
    "    for count, center in enumerate(centers):\n",
    "        # distance between the particles and each landmark\n",
    "        dist_p_l = np.linalg.norm(particles-center, axis=1, keepdims=True) - radii[count]\n",
    "        \n",
    "        # have tried use exponential function to avoid underflow, but still of no use\n",
    "        # since the distance is in the range of 1e2, so the possibility is 0 everywhere\n",
    "        # so here use scale_fac and std to avoid the underflow of possibilities\n",
    "        # set the distance as mean and std as standard deviation of norm distribution, then get the pdf as our new weights\n",
    "        weights *= stats.norm.pdf(dist_p_l/scale_fac, dist_r_l[count]/scale_fac, std)\n",
    "\n",
    "    weights += 1.e-100   # avoid round-off to zero\n",
    "    weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Procedures\n",
    "Discard highly impossible particles and replace them with copies of the more possible particles. Accordingly, particles with greater weights survive with higher likelihood than particles with valus of small importance. In principle, there are many ways to achieve this, here you can refer to the procedure given in the paper [Resampling in Particle Filtering - Comparison](http://sait.cie.put.poznan.pl/38/SAIT_38_02.pdf) to complete the systematic resampling method, in which the algorithm is described as below:\n",
    "<img src=\"img/systematic_resample.png\" alt=\"Encoder\" style=\"width: 400px;\"/>\n",
    "This resampling has a complexity of *O(N)*, and is one of the more readily recommended, because of its simplicity and operation speed. This approach assumes that the range $[0, 1)$ is subdivided in to N equal parts, and the draw occurs in each stratum\n",
    "$u^i \\sim [\\frac{i-1}{N}, \\frac{i}{N})$, particles are selected for replication, such that $u^i \\in [\\sum_p^{j-1}q_p, \\sum_p^{j}q_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_procedure(weights):\n",
    "    \"\"\"Perform resampling procedure described above\n",
    "    \n",
    "    Args:\n",
    "        weights: the weights to be updated\n",
    "    \n",
    "    ReturnL:\n",
    "        idx: the indices of those remained particled\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    num_weights = len(weights)\n",
    "    \n",
    "    # make N subdivisions, choose positions with a consistent random offset\n",
    "    U = (np.arange(num_weights) + np.random.uniform()) / num_weights\n",
    " \n",
    "    idx = np.zeros(num_weights, 'i') # set the data type as int\n",
    "    sum_Q = np.cumsum(weights)\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    while i<num_weights and j<num_weights:\n",
    "        if U[i] < sum_Q[j]:\n",
    "            idx[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above takes an array of weights and returns indexes of particles that have been chosen. We just need to write a function that performs the resampling from these indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, idx):\n",
    "    particles[:] = particles[idx]\n",
    "    weights[:] = weights[idx]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put the prediction positions of these particles together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the origin data as reference\n",
    "# you should not touch this data in you implementation\n",
    "input_sequence = np.load('./archive/Trajectory_1.npy')\n",
    "# Now let's input the velocity and distance data\n",
    "# So should be intepreted as corresponding transition and observability matrix in HMM type\n",
    "angle_velocity = np.load('./data/velocity_1.npy')\n",
    "dist_r_l = np.load('./data/distance_1.npy')\n",
    "# Now need to figure out what can be done next if you have velocity and diatance data at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization\n",
    "Now we visualize the moving of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparametes\n",
    "original_particle_number = 500\n",
    "gaussian_particle_std = 100\n",
    "\n",
    "# add noise to the distance prediction\n",
    "dis_std = 2\n",
    "\n",
    "# decide the convergence of particles\n",
    "weights_scale = 50 \n",
    "weights_std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we get the particles\n",
    "random_particles = uniform_particles_construction(width_max, height_max, original_particle_number)\n",
    "reject_random_particles = rejection_particles(random_particles, landmark_centers, landmark_radii)\n",
    "origin_random_weights = np.ones((len(reject_random_particles),1))\n",
    "\n",
    "# Now we need to record the coordinates of moving particles\n",
    "prediction_random_paticles = [reject_random_particles]\n",
    "\n",
    "random_pos = copy.copy(reject_random_particles)\n",
    "random_weights = copy.copy(origin_random_weights)\n",
    "\n",
    "for i in range(len(dist_r_l)):\n",
    "\n",
    "    # The predicted position of particles\n",
    "    predict_particle_pos(random_pos, angle_velocity[i], dis_std, 0.5)\n",
    "    weights_update(random_pos, random_weights, dist_r_l[i], landmark_centers, landmark_radii, weights_scale, weights_std)\n",
    "    random_index = resample_procedure(random_weights)\n",
    "    resample_from_index(random_pos, random_weights, random_index)\n",
    "    prediction_random_paticles.append(copy.copy(random_pos))\n",
    "    \n",
    "particle_random_trajectory = np.asarray(prediction_random_paticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(step):\n",
    "    img = plt.imread('img/Canvas.png')\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Now, loop through coord arrays, and create a circle at center\n",
    "    for count, value in enumerate(landmark_centers):\n",
    "        circ = Circle(value,landmark_radii[count])\n",
    "        ax.add_patch(circ)\n",
    "    \n",
    "    ax.scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax.scatter(particle_random_trajectory[step,:,0], particle_random_trajectory[step,:,1],s=1, c='g')\n",
    "    \n",
    "    plt.xlim(0, width_max) \n",
    "    plt.ylim(0, height_max)\n",
    "\n",
    "    # Create a figure. Equal aspect so circles look circular\n",
    "    ax.set_aspect('equal')\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=14, description='step', max=28), Output()), _dom_classes=('widget-interaâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4745e34ca044aa6a66a08793078b243"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "iplot = interactive(location, step=(0, len(particle_random_trajectory)-1))\n",
    "iplot"
   ]
  },
  {
   "source": [
    "show different version of noise, and also motion with no noise at  all\n",
    "from my side, it's the point that, when drawing sampling randomly in 2D space, since we have no idea about the original point of the robot, then it's something like find minimul value using gradient descent, and the noise in this scene corresponds to random walks, which can help to rescape from stucking at the local mininum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the robot starts at zero point, but the particles sampled randomly in the 2D spaces rarely go close to the zero, hence cause the bias which exists all over the trajectory. So instead, let's try with the Gaussian sample menthod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_particles = gaussian_particles_construction(width_max, height_max, original_particle_number, gaussian_particle_std)\n",
    "reject_gaussian_particles = rejection_particles(gaussian_particles, landmark_centers, landmark_radii)\n",
    "origin_gaussian_weights = np.ones((len(reject_gaussian_particles),1))\n",
    "\n",
    "# Now we need to record the coordinates of moving particles\n",
    "prediction_gaussian_paticles = [reject_gaussian_particles]\n",
    "\n",
    "gaussian_pos = copy.copy(reject_gaussian_particles)\n",
    "gaussian_weights = copy.copy(origin_gaussian_weights)\n",
    "\n",
    "for i in range(len(input_sequence)-1):\n",
    "\n",
    "    predict_particle_pos(gaussian_pos, angle_velocity[i], dis_std, 0.5)\n",
    "    weights_update(gaussian_pos, gaussian_weights, dist_r_l[i], landmark_centers, landmark_radii, weights_scale, weights_std)\n",
    "    gaussian_index = resample_procedure(gaussian_weights)\n",
    "    resample_from_index(gaussian_pos, gaussian_weights, gaussian_index)\n",
    "    prediction_gaussian_paticles.append(copy.copy(gaussian_pos))\n",
    "    \n",
    "particle_gaussian_trajectory = np.asarray(prediction_gaussian_paticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(step):\n",
    "    img = plt.imread('img/Canvas.png')\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Now, loop through coord arrays, and create a circle at center\n",
    "    for count, value in enumerate(landmark_centers):\n",
    "        circ = Circle(value,landmark_radii[count])\n",
    "        ax.add_patch(circ)\n",
    "    \n",
    "    ax.scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax.scatter(particle_gaussian_trajectory[step,:,0], particle_gaussian_trajectory[step,:,1],s=1, c='g')\n",
    "    \n",
    "    plt.xlim(0, width_max) \n",
    "    plt.ylim(0, height_max)\n",
    "\n",
    "    # Create a figure. Equal aspect so circles look circular\n",
    "    ax.set_aspect('equal')\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0be0e367e884942a30a0f7a9a05fd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14, description='step', max=28), Output()), _dom_classes=('widget-interaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot = interactive(location, step=(0, len(particle_gaussian_trajectory)-1))\n",
    "iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's predict the position of robot's final position and make comparison with the true result. The final position should be somewhere around ...(tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate position using random sampled particles is [416.49524062 227.03834284]\n",
      "The estimate position using Gaussian sampled particles is [405.28422945 235.12199051]\n",
      "The true final position of robot is [420.31088083 217.12707182]\n"
     ]
    }
   ],
   "source": [
    "predict_final_random = np.average(particle_random_trajectory[-1], weights=random_weights.flatten(), axis=0)\n",
    "print(\"The estimate position using random sampled particles is {}\".format(predict_final_random))\n",
    "predict_final_gaussian = np.average(particle_gaussian_trajectory[-1], weights=gaussian_weights.flatten(), axis=0)\n",
    "print(\"The estimate position using Gaussian sampled particles is {}\".format(predict_final_gaussian))\n",
    "print(\"The true final position of robot is {}\".format(input_sequence[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}